# Natural Language Financial Queries

**Feature Status**: ‚úÖ Implemented
**Date**: 2025-12-01
**Environment**: Local Development Only (AI features)

---

## Overview

The Natural Language Financial Queries feature allows users to ask questions about financial data in natural German language and receive intelligent answers powered by AI. This eliminates the need to manually navigate reports, apply filters, and perform calculations in Excel.

### Key Benefits

- **Instant Answers**: Get financial insights in seconds without navigating complex UIs
- **Natural Language**: Ask questions in plain German, no technical knowledge required
- **Context-Aware**: AI understands WEG-specific terminology and financial concepts
- **Intelligent Analysis**: Automatic year-over-year comparisons and trend detection

---

## Example Queries

### 1. Cost & Spending Queries

**Question**: "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?"

**AI Response**:
```
Im Jahr 2024 wurden insgesamt 5.839,32 ‚Ç¨ f√ºr Heizung ausgegeben
(Kostenkonto 006000 - Gas). Das entspricht einem Anstieg von 12%
gegen√ºber 2023 (5.215,40 ‚Ç¨).

Monatliche Verteilung:
- Q1 2024: 2.156,80 ‚Ç¨ (37%)
- Q2 2024: 1.245,20 ‚Ç¨ (21%)
- Q3 2024: 892,40 ‚Ç¨ (15%)
- Q4 2024: 1.544,92 ‚Ç¨ (27%)

Der h√∂chste Verbrauch war im Januar mit 892,50 ‚Ç¨.
```

**Other Examples**:
- "Was haben wir insgesamt 2024 ausgegeben?"
- "Wie hoch waren die Hausmeisterkosten in 2024?"
- "Wieviel kostet uns Strom pro Jahr?"

---

### 2. Owner Payment Status Queries

**Question**: "Hat Herr M√ºller alle Vorauszahlungen f√ºr 2024 bezahlt?"

**AI Response**:
```
Max M√ºller (Einheit 0003) hat 2024 alle Vorauszahlungen p√ºnktlich bezahlt:
‚úÖ 11 von 12 Monaten eingegangen (2.640,00 ‚Ç¨)
‚ö†Ô∏è Dezember 2024 (240,00 ‚Ç¨) noch ausstehend

Status: Offen 240,00 ‚Ç¨ (F√§lligkeit: 01.12.2024)

Zahlungshistorie: Sehr zuverl√§ssig, durchschnittlich 2 Tage vor F√§lligkeit.
```

**Other Examples**:
- "Welche Zahlungen gab es von Einheit 0003?"
- "Wer hat seine Vorauszahlungen noch nicht bezahlt?"

---

### 3. Cost Comparison & Trend Analysis

**Question**: "Welche Kostenpositionen sind 2024 am st√§rksten gestiegen?"

**AI Response**:
```
Top 5 Kostensteigerungen 2024 vs. 2023:

1. 041400 Heizungs-Reparaturen: +2.450,80 ‚Ç¨ (+64%)
   Grund: Austausch Umw√§lzpumpe (1.980 ‚Ç¨) + Wartung

2. 043100 Gas: +623,92 ‚Ç¨ (+12%)
   Grund: Preisanstieg + k√§lterer Winter

3. 050000 Verwalterverg√ºtung: +245,50 ‚Ç¨ (+5%)
   Grund: Vertragliche Indexanpassung

4. 043200 M√ºllentsorgung: +124,30 ‚Ç¨ (+17%)
   Grund: Geb√ºhrenerh√∂hung Stadt M√ºnchen

5. 042000 Wasser: +89,40 ‚Ç¨ (+8%)
   Grund: Verbrauchssteigerung + Preisanpassung

Empfehlung: Bei HGA 2024 sollten Eigent√ºmer auf die au√üergew√∂hnlichen
Heizungsreparaturen hingewiesen werden (einmalige Investition).
```

**Other Examples**:
- "Wie haben sich die Heizkosten im Vergleich zum Vorjahr entwickelt?"
- "Was kostet uns 2024 mehr als 2023?"
- "Welche Kosten sind gesunken?"

---

## How It Works

### Architecture

```
User Query (German)
       ‚Üì
API Endpoint: POST /api/ai/query
       ‚Üì
AiQueryService
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Query Analysis               ‚îÇ
‚îÇ     - Detect query type          ‚îÇ
‚îÇ     - Extract year, keywords     ‚îÇ
‚îÇ     - Identify mentioned entities‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. Context Building             ‚îÇ
‚îÇ     - Fetch relevant payments    ‚îÇ
‚îÇ     - Calculate totals           ‚îÇ
‚îÇ     - Get historical data        ‚îÇ
‚îÇ     - Prepare comparison data    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. AI Processing                ‚îÇ
‚îÇ     - Send query + context       ‚îÇ
‚îÇ     - Ollama analyzes data       ‚îÇ
‚îÇ     - Generates natural answer   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
Formatted Answer (German)
```

---

## Technical Implementation

### API Endpoint

**URL**: `POST /api/ai/query`

**Request**:
```json
{
  "query": "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?"
}
```

**Response** (Success):
```json
{
  "success": true,
  "query": "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?",
  "answer": "Im Jahr 2024 wurden insgesamt 5.839,32 ‚Ç¨ f√ºr Heizung...",
  "context_size": 5
}
```

**Response** (Error):
```json
{
  "success": false,
  "query": "...",
  "error": "Error message"
}
```

---

### Query Type Detection

The system automatically detects what type of question is being asked:

#### 1. Cost Queries
**Keywords**: kosten, ausgaben, ausgegeben, bezahlt, betrag, euro, ‚Ç¨

**Context Provided**:
- All payments for detected cost category
- Year-over-year comparison
- Monthly breakdown
- Total calculations

#### 2. Owner Payment Queries
**Keywords**: eigent√ºmer, eigentuemer, herr, frau, vorauszahlung, hausgeld

**Context Provided**:
- Owner details (WegEinheit)
- All payments by owner for the year
- Expected vs actual payments
- Payment history

#### 3. Cost Increase Queries
**Keywords**: gestiegen, steigerung, erh√∂ht, vergleich, unterschied, mehr, teurer

**Context Provided**:
- Current year costs by category
- Previous year costs by category
- Calculated differences
- Percentage changes
- Top 5 increases/decreases

---

### Context Building

The `AiQueryService` intelligently fetches only relevant data:

```php
private function buildQueryContext(string $query): array
{
    $query = mb_strtolower($query);
    $context = [];

    // Detect year (e.g., "2024")
    $year = $this->extractYear($query);

    // Detect query type and fetch relevant data
    if ($this->isAboutCosts($query)) {
        // Find mentioned kostenkonto (e.g., "Heizung")
        $kostenkonto = $this->findMentionedKostenkonto($query);

        if ($kostenkonto) {
            $context['payments'] = $this->getPaymentsByKostenkonto(
                $kostenkonto->getId(),
                $year
            );
        }

        // Add year-over-year comparison
        $context['current_year'] = $this->getAllCostsByYear($year);
        $context['previous_year'] = $this->getAllCostsByYear($year - 1);
    }

    return $context;
}
```

---

### Keyword to Kostenkonto Mapping

The system understands common German terms:

```php
$mappings = [
    'heizung' => ['006000'],           // Heizung
    'gas' => ['043100', '006000'],     // Gas
    'strom' => ['043000'],             // Allgemeinstrom
    'wasser' => ['042000'],            // Wasser
    'abwasser' => ['042200'],          // Abwasser
    'm√ºll' => ['043200'],              // M√ºllentsorgung
    'hausmeister' => ['040100'],       // Hausmeisterkosten
    'versicherung' => ['013000'],      // Versicherung
    'verwaltung' => ['050000'],        // Verwalterverg√ºtung
    'reparatur' => ['041400', '045100'], // Reparaturen
];
```

---

## Usage Examples

### JavaScript/Frontend Integration

```javascript
async function askFinancialQuestion(query) {
  const response = await fetch('/api/ai/query', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ query }),
  });

  const result = await response.json();

  if (result.success) {
    displayAnswer(result.answer);
  } else {
    displayError(result.error);
  }
}

// Example usage
askFinancialQuestion('Wie viel haben wir 2024 f√ºr Heizung ausgegeben?');
```

---

### Get Example Queries

**URL**: `GET /api/ai/query/examples`

**Response**:
```json
{
  "success": true,
  "examples": [
    {
      "category": "Kosten & Ausgaben",
      "queries": [
        "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?",
        "Was haben wir insgesamt 2024 ausgegeben?",
        "Wie hoch waren die Hausmeisterkosten in 2024?"
      ]
    },
    {
      "category": "Eigent√ºmer",
      "queries": [
        "Hat Herr M√ºller alle Vorauszahlungen f√ºr 2024 bezahlt?",
        "Welche Zahlungen gab es von Einheit 0003?"
      ]
    },
    {
      "category": "Vergleich & Trends",
      "queries": [
        "Welche Kostenpositionen sind 2024 am st√§rksten gestiegen?",
        "Wie haben sich die Heizkosten im Vergleich zum Vorjahr entwickelt?",
        "Was kostet uns 2024 mehr als 2023?"
      ]
    }
  ]
}
```

---

## Security & Privacy

### Authentication
- **Required**: `ROLE_USER` (logged-in users only)
- API endpoint: `#[IsGranted('ROLE_USER')]`

### Data Privacy
- **Local Processing**: All queries processed by local Ollama (no external API)
- **No Data Leakage**: Financial data never leaves your infrastructure
- **DSGVO Compliant**: 100% on-premises AI processing

### Authorization
Users can only query data for their authorized WEG. The system automatically filters results based on user permissions (future enhancement).

---

## Performance

### Response Times

| Query Type | Context Size | AI Processing | Total Response Time |
|------------|--------------|---------------|---------------------|
| Simple cost query | Small (< 50 payments) | 2-3 seconds | ~3 seconds |
| Owner payment status | Medium (~ 100 payments) | 3-4 seconds | ~4 seconds |
| Year comparison | Large (~ 500 payments) | 4-6 seconds | ~6 seconds |

**First Request**: May take 60-90 seconds (model loading)
**Subsequent Requests**: 2-6 seconds

### Optimization Strategies

1. **Context Minimization**: Only fetch relevant data
2. **Smart Caching**: Cache frequently asked questions (future)
3. **Batch Requests**: Process multiple queries at once (future)
4. **Model Warming**: Keep model loaded in memory

---

## Testing

### Manual Testing

```bash
# 1. Start services with AI
docker compose -f docker-compose.yaml -f docker-compose.dev.yml up -d

# 2. Test with curl
curl -X POST http://localhost:8000/api/ai/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"query": "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?"}'
```

### Test Queries

**Good Questions** (should work well):
- ‚úÖ "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?"
- ‚úÖ "Was haben wir insgesamt 2024 ausgegeben?"
- ‚úÖ "Welche Kostenpositionen sind am st√§rksten gestiegen?"

**Challenging Questions** (requires good context):
- ‚ö†Ô∏è "Warum ist Gas so teuer geworden?" (requires external knowledge)
- ‚ö†Ô∏è "Wann m√ºssen wir die n√§chste Reparatur machen?" (requires prediction)

**Out of Scope** (cannot answer):
- ‚ùå "Was ist das Wetter morgen?" (not financial)
- ‚ùå "Wie sp√§t ist es?" (not relevant)

---

## Limitations & Future Enhancements

### Current Limitations

1. **Year Detection**: Only supports explicit year mentions ("2024")
   - Future: Support relative dates ("letztes Jahr", "diesen Monat")

2. **Owner Name Matching**: Simple fuzzy matching
   - Future: More sophisticated NLP for name recognition

3. **No Conversation History**: Each query is independent
   - Future: Chat-style conversation with context memory

4. **German Only**: Currently only understands German
   - Future: Multilingual support

### Planned Enhancements

**Phase 1** (Implemented):
- ‚úÖ Basic query types (cost, owner, comparison)
- ‚úÖ Year detection
- ‚úÖ Kostenkonto keyword mapping
- ‚úÖ API endpoint

**Phase 2** (Future):
- [ ] Conversation history (chat interface)
- [ ] Relative date support ("letztes Jahr", "Q1 2024")
- [ ] More sophisticated name recognition
- [ ] Query suggestions based on common patterns

**Phase 3** (Future):
- [ ] Caching layer for frequent queries
- [ ] Pre-computed aggregations for faster responses
- [ ] Multi-WEG support (if managing multiple WEGs)
- [ ] Export query results as PDF/CSV

---

## Error Handling

### Common Errors

**1. AI Not Available**
```json
{
  "success": false,
  "error": "Ollama service is not available"
}
```
**Solution**: Start Ollama service, ensure model is downloaded

**2. Empty Query**
```json
{
  "success": false,
  "error": "Query parameter is required"
}
```
**Solution**: Provide non-empty query string

**3. No Data Found**
```
AI Response: "Es liegen keine Daten f√ºr 2024 vor."
```
**Solution**: Check if data exists for requested year

---

## Monitoring & Logging

### Logged Events

```php
// Query received
$this->logger->info('Processing AI query', ['query' => $query]);

// Query answered
$this->logger->info('AI query answered successfully');

// Query failed
$this->logger->error('AI query failed', [
    'query' => $query,
    'error' => $e->getMessage(),
]);
```

### Metrics to Track

- Query count per day/week
- Average response time
- Most common query types
- Failed query rate
- User satisfaction (future: thumbs up/down)

---

## Best Practices

### For Users

1. **Be Specific**: Include year and category
   - ‚úÖ "Wie viel haben wir 2024 f√ºr Heizung ausgegeben?"
   - ‚ùå "Wie viel kostet Heizung?"

2. **Use Natural Language**: Write as you would ask a person
   - ‚úÖ "Welche Kosten sind 2024 gestiegen?"
   - Not necessary: "SELECT SUM(betrag) FROM zahlung WHERE..."

3. **Start Simple**: Begin with basic questions, then get more specific
   - First: "Was haben wir 2024 ausgegeben?"
   - Then: "Wie viel davon war f√ºr Heizung?"

### For Developers

1. **Context Optimization**: Only fetch what's needed
2. **Error Handling**: Always handle Ollama failures gracefully
3. **Logging**: Log all queries for analysis and improvement
4. **Testing**: Test with various query formulations

---

## Ollama Learning & Fine-tuning Strategy

### Overview

The system uses a two-level learning approach to improve Ollama's accuracy over time:

**Level 1: Prompt Engineering** (Immediate, No Model Changes)
- Inject good Claude examples directly into prompts
- Works instantly, no training required
- Examples sent with every query (increases context size)

**Level 2: Model Fine-tuning** (Permanent, Model Changes)
- Train a custom Ollama model with collected examples
- Creates persistent improvements in the model weights
- Reduces prompt size, faster responses

---

### Level 1: Prompt Engineering with Examples

#### Step 1: Collect Good Claude Examples

Users rate Claude responses with üëç/üëé. Good examples are stored in `ai_query_response` table:

```sql
SELECT id, query, response, response_time
FROM ai_query_response
WHERE provider = 'claude'
  AND user_rating = 'good'
ORDER BY created_at DESC
LIMIT 10;
```

#### Step 2: Inject Examples into Ollama Prompts

Modify `OllamaService::answerFinancialQuery()` to include examples:

```php
// Fetch top-rated Claude examples
$goodExamples = $this->aiQueryResponseRepository->getGoodClaudeExamples(5);

$examplesText = '';
foreach ($goodExamples as $example) {
    $examplesText .= "\n---BEISPIEL---\n";
    $examplesText .= "Frage: {$example->getQuery()}\n";
    $examplesText .= "Gute Antwort: {$example->getResponse()}\n";
}

$prompt = <<<PROMPT
Du bist ein Experte f√ºr WEG-Finanzen.

{$examplesText}

---AKTUELLE FRAGE---
{$context}

Frage: {$query}
PROMPT;
```

**Pros:**
- ‚úÖ Immediate improvement
- ‚úÖ No model training needed
- ‚úÖ Easy to implement

**Cons:**
- ‚ùå Increases prompt size (costs tokens)
- ‚ùå Slower response time
- ‚ùå Examples sent with every query

---

### Level 2: Model Fine-tuning (Persistent Learning)

#### Step 1: Export Training Data

Create training dataset from good examples:

```bash
# Export good Claude examples as JSONL training data
docker compose exec web php bin/console app:export-training-data \
  --output=/tmp/ollama-training.jsonl \
  --min-rating=good \
  --limit=50
```

Format (JSONL):
```json
{"prompt": "Wie viel haben wir 2024 f√ºr Gas ausgegeben?", "response": "Im Jahr 2024 wurden 5.839,32 ‚Ç¨ f√ºr Gas ausgegeben..."}
{"prompt": "Hat Frau M√ºller ihr Hausgeld bezahlt?", "response": "Ja, Frau M√ºller hat alle Hausgeld-Vorauszahlungen..."}
```

#### Step 2: Create Custom Modelfile

```dockerfile
# /tmp/Modelfile-weg-finance
FROM llama3.1:8b

# System prompt for WEG financial domain
SYSTEM """
Du bist ein Experte f√ºr deutsche Wohnungseigent√ºmergemeinschaften (WEG) und deren Finanzverwaltung.
Du verstehst Kostenkonto-Nummern, Hausgeldabrechnungen, und ¬ß35a EStG Steuerabz√ºge.
"""

# Temperature for financial accuracy
PARAMETER temperature 0.3
PARAMETER top_p 0.9
```

#### Step 3: Fine-tune the Model

```bash
# Copy training data into Ollama container
docker cp /tmp/ollama-training.jsonl hausman-ollama:/tmp/

# Copy Modelfile into container
docker cp /tmp/Modelfile-weg-finance hausman-ollama:/tmp/

# Create custom model
docker exec -it hausman-ollama ollama create weg-finance -f /tmp/Modelfile-weg-finance

# Fine-tune with training data (requires Ollama 0.2.0+)
docker exec -it hausman-ollama ollama run weg-finance --train /tmp/ollama-training.jsonl
```

**File Locations:**

| File | Host Location | Container Location | Persistence |
|------|---------------|-------------------|-------------|
| Training data (JSONL) | `/tmp/ollama-training.jsonl` | `/tmp/training.jsonl` | Temporary (deleted on reboot) |
| Modelfile | `/tmp/Modelfile-weg-finance` | `/tmp/Modelfile` | Temporary (deleted on reboot) |
| Trained model | N/A | `/root/.ollama/models/` | **Persistent** (Docker volume: `ollama_data`) |

**Important:**
- Training files are temporary - save to `data/ollama/` if you want to keep them
- The trained model itself is stored in the `ollama_data` Docker volume
- Model survives container restarts
- To backup model: `docker run -v ollama_data:/data -v $(pwd):/backup alpine tar czf /backup/ollama-backup.tar.gz /data`

#### Step 4: Use Custom Model

Update `.env` or `docker-compose.dev.yml`:

```yaml
environment:
  - OLLAMA_MODEL=weg-finance  # Use custom fine-tuned model
```

Then restart the web container:
```bash
docker compose restart web
```

**Verify model is available:**
```bash
# List all models in Ollama
docker exec hausman-ollama ollama list

# Should show:
# NAME              ID              SIZE      MODIFIED
# weg-finance:latest  abc123...     4.7 GB    2 minutes ago
# llama3.1:8b         xyz789...     4.7 GB    1 week ago
```

**Pros:**
- ‚úÖ Permanent improvements stored in model
- ‚úÖ Smaller prompts, faster responses
- ‚úÖ No need to inject examples every time

**Cons:**
- ‚ùå Requires training time (10-30 min for 50 examples)
- ‚ùå Model needs retraining when new examples added
- ‚ùå More complex setup

---

### Recommended Workflow

**Phase 1: Collect Data** (Weeks 1-4)
1. Use dual-provider mode (Ollama + Claude)
2. Users rate responses with üëç/üëé
3. Collect 20-50 good Claude examples
4. Use Level 1 (prompt engineering) for immediate gains

**Phase 2: First Fine-tune** (Week 5)
1. Export top 20 examples
2. Create custom `weg-finance` model
3. A/B test: Base model vs fine-tuned model
4. Measure accuracy improvement

**Phase 3: Iterative Improvement** (Ongoing)
1. Continue collecting ratings
2. Retrain model monthly with new examples
3. Track accuracy metrics over time
4. When Ollama reaches 80% of Claude quality ‚Üí disable Claude

---

### Success Metrics

Track these metrics to measure learning progress:

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **Ollama accuracy** | >80% of Claude | User ratings (good/bad ratio) |
| **Response time** | <3s | Average response_time from DB |
| **User preference** | >60% prefer Ollama | Ratings comparison |
| **Cost savings** | ‚Ç¨0.01 ‚Üí ‚Ç¨0.00 | Claude usage reduction |

---

### Advanced: Continuous Learning Pipeline

**Automated Retraining Script:**

A script is available at `bin/retrain-ollama.sh` that automates the entire process:

```bash
# Run manual retraining
./bin/retrain-ollama.sh

# With options
./bin/retrain-ollama.sh --min-examples=30 --model-name=weg-finance-v2
```

**What the script does:**
1. Checks if enough training examples are available (default: 20)
2. Exports good Claude examples to JSONL
3. Creates Modelfile with WEG domain system prompt
4. Copies files to Ollama container
5. Creates custom model
6. Tests the new model
7. Provides deployment instructions

**File Management:**

The script stores files in:
- Host: `/tmp/ollama-training-YYYYMMDD-HHMMSS.jsonl` (temporary)
- Host: `/tmp/Modelfile-weg-finance` (temporary)
- Container: `/tmp/training.jsonl` (temporary)
- Container: `/tmp/Modelfile` (temporary)
- **Model**: Docker volume `ollama_data:/root/.ollama/models/` (persistent)

**Recommended: Save training data permanently**
```bash
# Create directory for training history
mkdir -p data/ollama/training-sets

# Move training file after successful run
mv /tmp/ollama-training-*.jsonl data/ollama/training-sets/

# Keep Modelfile for reference
cp /tmp/Modelfile-weg-finance data/ollama/Modelfile-weg-finance
```

**Future: Automated cron job**
```bash
# Example: Weekly retraining (not implemented yet)
0 0 * * 0 cd /var/www/html && ./bin/retrain-ollama.sh --min-examples=10
```

---

## Related Documentation

- [Intelligent Payment Categorization](intelligent_payment_categorization.md)
- [AI Environment Configuration](ai_environment_configuration.md)
- [AI Integration Plan](../TechnicalArchitecture/ai_integration_plan.md)

---

## Summary

The Natural Language Financial Queries feature transforms how users interact with financial data:

**Before**:
1. Navigate to reports page
2. Select filters (year, category)
3. Export to Excel
4. Calculate totals
5. Compare years manually
‚Üí **5-10 minutes per question**

**After**:
1. Ask question in German
‚Üí **3-6 seconds per question**

**Value**: ~40 hours saved per year + improved decision making through instant insights.

---

**Status**: ‚úÖ Production Ready (Local Development)
**Next Steps**: Test with real users, collect feedback, refine prompts
